{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 0
   },
   "source": [
    "# Phase 4 Code Challenge Review"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- Pipelines and gridsearching\n",
    "- Ensemble Methods\n",
    "- Natural Language Processing\n",
    "- Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.call import call_on_students"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Pipelines and Gridsearching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the benefits of using a pipline?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pipeline in data science and machine learning workflows offers several benefits:\n",
    "\n",
    "1. Streamlined Workflow: Pipelines provide a structured and organized approach to data processing and model building. They allow you to define and automate a sequence of data transformation steps, such as data preprocessing, feature engineering, model training, and prediction, in a coherent manner. This streamlines the workflow and makes it easier to reproduce and maintain.\n",
    "\n",
    "2. Code Readability and Maintainability: Pipelines improve the readability and maintainability of your code. By encapsulating the steps within a pipeline, it becomes easier to understand the flow of data and operations performed. This modular structure makes it simpler to modify or update specific steps without impacting the entire workflow.\n",
    "\n",
    "3. Data Leakage Prevention: Pipelines help prevent data leakage, which occurs when information from the test set inadvertently influences the model during training or preprocessing. By explicitly separating the steps into distinct stages within the pipeline, you can ensure that each stage operates on the appropriate subset of data, avoiding data leakage and producing more reliable model evaluations.\n",
    "\n",
    "4. Hyperparameter Tuning: Pipelines can be combined with techniques like grid search or random search for hyperparameter tuning. This allows you to systematically explore different combinations of hyperparameters for your models within the pipeline. By automating this process, you can find the best set of hyperparameters that maximize model performance without manual intervention.\n",
    "\n",
    "5. Model Deployment and Scalability: Pipelines facilitate the deployment and scalability of your models. Once you have defined a pipeline for your data processing and modeling steps, it becomes easier to deploy the entire pipeline as a cohesive unit. This is particularly beneficial when deploying models in production environments. Moreover, pipelines can handle large datasets efficiently, enabling scalability and parallel processing of data and models.\n",
    "\n",
    "6. Reproducibility: Pipelines promote reproducibility by capturing the entire data processing and modeling workflow in a single entity. This ensures that the same transformations and modeling steps are consistently applied to new data or in future runs. By storing the pipeline configuration or using pipeline serialization, you can recreate the exact data processing and modeling pipeline, making it easier to reproduce results and share them with others.\n",
    "\n",
    "Overall, pipelines enhance the efficiency, reliability, and maintainability of data science workflows by providing a structured framework for data processing, modeling, and deployment. They contribute to better code organization, prevent data leakage, enable hyperparameter tuning, and support reproducibility, ultimately leading to more robust and scalable data science projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What does a gridsearch achieve?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search is a hyperparameter tuning technique used in machine learning to find the optimal combination of hyperparameter values for a given model. Hyperparameters are parameters that are set before the learning process begins and control the behavior of the model.\n",
    "\n",
    "The goal of grid search is to systematically search through a predefined grid of hyperparameter values and evaluate the model's performance using each combination of values. It exhaustively tries all possible combinations within the specified grid and identifies the combination that produces the best performance according to a chosen evaluation metric, such as accuracy, precision, recall, or F1-score.\n",
    "\n",
    "Here's how grid search works:\n",
    "\n",
    "1. Define the Hyperparameter Grid: Specify a grid of hyperparameter values to explore for each hyperparameter of the model. For example, if you have two hyperparameters, A and B, with possible values [1, 2, 3] and [0.1, 0.2, 0.3], respectively, the grid search will create a Cartesian product of these values, resulting in nine combinations to evaluate.\n",
    "\n",
    "2. Model Training and Evaluation: For each combination of hyperparameter values, train the model using the training dataset and evaluate its performance using a validation dataset or through cross-validation. The evaluation metric is calculated to assess how well the model performs with the given hyperparameter values.\n",
    "\n",
    "3. Best Hyperparameters Selection: Compare the performance of each model based on the evaluation metric and select the combination of hyperparameters that achieves the best performance. This is typically the combination that maximizes the evaluation metric, although it can also be based on minimizing a loss function, error, or other relevant metrics.\n",
    "\n",
    "By exhaustively searching through the grid of hyperparameters, grid search helps identify the hyperparameter values that lead to the best model performance on the validation dataset. It automates the process of finding the optimal hyperparameter values, saving time and effort compared to manually testing different combinations.\n",
    "\n",
    "Grid search is implemented in various machine learning libraries, such as scikit-learn in Python, making it straightforward to apply in practice. It is a valuable technique for improving the performance of machine learning models and finding the best hyperparameter configuration, resulting in more accurate and reliable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Set up a pipeline with a scaler and a logistic regression model on the breast cancer dataset that predicts whether the tumor is malignant (target = 1). Don't worry for now about a train-test split."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9876977152899824"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "pipe1 = Pipeline([('scaler', StandardScaler()),\n",
    "                  ('logreg', LogisticRegression()),\n",
    "                  ])\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "pipe1.fit(X, y)\n",
    "pipe1.score(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Split the data into train and test and then gridsearch over pipelines like the one you just built to find the best-performing model. Try C (inverse regularization) values of 10, 1, and 0.1. Try out the best estimator on the test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'logreg__C': [10, 1, 0.1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe1,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;logreg&#x27;, LogisticRegression())]),\n",
       "             param_grid={&#x27;logreg__C&#x27;: [10, 1, 0.1]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;logreg&#x27;, LogisticRegression())]),\n",
       "             param_grid={&#x27;logreg__C&#x27;: [10, 1, 0.1]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;logreg&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('logreg', LogisticRegression())]),\n",
       "             param_grid={'logreg__C': [10, 1, 0.1]})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'logreg__C': 10}\n",
      "Best Score: 0.9764705882352942\n",
      "Test Accuracy: 0.972027972027972\n"
     ]
    }
   ],
   "source": [
    "# import accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Access the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Apply the best estimator to the test set\n",
    "best_estimator = grid_search.best_estimator_\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "# Evaluate the performance on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Ensemble Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What sorts of ensembling methods have we looked at?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baging \n",
    "random forests\n",
    "boosting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ensemble methods include:\n",
    "\n",
    "1. Random Forest: It combines multiple decision trees by using random sampling of training data and random feature selection to improve predictive accuracy and reduce overfitting.\n",
    "\n",
    "2. Gradient Boosting: It builds an ensemble of weak learners (usually decision trees) in a sequential manner. Each subsequent model corrects the errors made by the previous model, gradually improving the overall prediction.\n",
    "\n",
    "3. AdaBoost: It trains a sequence of weak learners in which each subsequent model focuses on the instances that were misclassified by previous models. The models are combined by giving more weight to the more accurate ones.\n",
    "\n",
    "4. Bagging: It involves training multiple instances of the same base model using bootstrap sampling and averaging the predictions to reduce variance and improve generalization.\n",
    "\n",
    "5. Stacking: It combines the predictions of multiple different models by training a meta-model (also known as a blender or meta-learner) on the predictions of the base models. The meta-model learns to make the final prediction based on the outputs of the base models.\n",
    "\n",
    "6. Voting: It combines predictions from multiple models by taking the majority vote (for classification problems) or averaging the predictions (for regression problems) to make the final prediction.\n",
    "\n",
    "7. Extra Trees: Similar to Random Forest, it combines multiple decision trees. However, Extra Trees further randomizes the tree construction process by selecting random splits at each node, aiming for even more diversity.\n",
    "\n",
    "These ensemble methods leverage the wisdom of multiple models to improve predictive performance, increase robustness, and handle complex patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is random about a random forest?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Random Forest is called \"random\" because it randomly selects subsets of the training data and features to build a collection of decision trees, promoting diversity and improving generalization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Random Forest, the \"random\" refers to two main aspects of the algorithm:\n",
    "\n",
    "    Random Sampling of Training Data:\n",
    "        Random Forest uses a technique called \"bootstrap aggregating\" or \"bagging.\" It involves randomly selecting subsets of the training data with replacement.\n",
    "        At each tree-building step, a random subset of the training data (with replacement) is used to train each decision tree in the forest. This random sampling helps to introduce diversity in the training data for each tree.\n",
    "\n",
    "    Random Feature Selection:\n",
    "        In addition to sampling the training data, Random Forest also performs random feature selection.\n",
    "        At each split of a decision tree, a random subset of features is considered as candidates for splitting. This helps to reduce the correlation among the trees and makes each tree focus on different subsets of features.\n",
    "        The number of features considered at each split is typically the square root of the total number of features.\n",
    "\n",
    "By incorporating these two randomization techniques, Random Forest aims to reduce overfitting and improve the model's ability to generalize well to unseen data. The random sampling of training data and random feature selection help to decorrelate the trees and capture different aspects of the data, resulting in an ensemble of diverse decision trees that work together to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What hyperparameters of a random forest might it be useful to tune? How so?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important hyperparameters to tune in a Random Forest are `n_estimators` (number of trees), `max_depth` (maximum depth of trees), `min_samples_split` and `min_samples_leaf` (minimum samples for splitting and leaf formation), `max_features` (number of features considered), and `bootstrap` (use of bootstrap sampling). Tuning these parameters helps balance model complexity and generalization, prevent overfitting, and improve performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When tuning a Random Forest model, some important hyperparameters to consider are:\n",
    "\n",
    "1. `n_estimators`: It represents the number of decision trees in the forest. Increasing the number of estimators generally improves the model's performance, but it also increases the computational cost.\n",
    "\n",
    "2. `max_depth`: It determines the maximum depth allowed for each decision tree in the forest. Deeper trees can capture complex relationships in the data but may overfit. Shallow trees can reduce overfitting but may result in underfitting. Tuning this parameter helps balance the trade-off between model complexity and generalization.\n",
    "\n",
    "3. `min_samples_split` and `min_samples_leaf`: These parameters control the minimum number of samples required to split an internal node or form a leaf node, respectively. Increasing these values can prevent overfitting by enforcing a minimum number of samples needed for a split or a leaf.\n",
    "\n",
    "4. `max_features`: It determines the number of features to consider when looking for the best split at each tree node. By limiting the number of features, you can reduce the correlation among the trees and introduce more randomness into the model.\n",
    "\n",
    "5. `bootstrap`: It specifies whether bootstrap sampling should be used when building trees. Setting this parameter to `True` (default) enables bootstrap sampling, while setting it to `False` disables it. Disabling bootstrap sampling can lead to more diversity in the trees but may result in higher variance.\n",
    "\n",
    "6. `random_state`: It is the random seed used for random number generation. Fixing the random state ensures reproducibility of the results.\n",
    "\n",
    "By tuning these hyperparameters, you can find the optimal configuration for your Random Forest model. It helps you strike a balance between overfitting and underfitting, improve model performance, and enhance generalization to unseen data. Hyperparameter tuning can be performed using techniques like grid search, random search, or Bayesian optimization to find the best combination of values for these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Build a random forest model on the breast cancer dataset that predicts whether the tumor is malignant (target = 1). Make sure you do a train-test split!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "pipe2 = Pipeline([('scaler', StandardScaler()),\n",
    "                  ('rf', RandomForestClassifier()),\n",
    "                  ])\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'rf__max_depth': [None, 5, 10],  # Maximum depth of trees\n",
    "    # Add more hyperparameters to tune as needed\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe2, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'rf__max_depth': 5, 'rf__n_estimators': 100}\n",
      "Best Score: 0.9624623803009577\n"
     ]
    }
   ],
   "source": [
    "# Fit the GridSearchCV object on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 23
   },
   "source": [
    "# 3) Natural Language Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Concepts\n",
    "\n",
    "### Some Example Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "index": 24
   },
   "outputs": [],
   "source": [
    "# Each sentence is a document\n",
    "sentence_one = \"Harry Potter is the best young adult book about wizards\"\n",
    "sentence_two = \"Um, EXCUSE ME! Ever heard of Earth Sea?\"\n",
    "sentence_three = \"I only like to read non-fiction.  It makes me a better person.\"\n",
    "\n",
    "# The corpus is composed of all of the documents\n",
    "corpus = [sentence_one, sentence_two, sentence_three]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: NLP Pre-processing\n",
    "\n",
    "List at least three steps you can take to turn raw text like this into something that would be semantically valuable (aka ready to turn into numbers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn raw text into something semantically valuable for numerical analysis, you can: clean and preprocess the text, normalize and lemmatize the words, and represent the text as numerical features using techniques like bag-of-words or TF-IDF."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn raw text into something semantically valuable and ready for numerical representation, you can follow these three steps:\n",
    "\n",
    "1. Text Cleaning and Preprocessing:\n",
    "   - Remove any unnecessary characters, punctuation, or special symbols that do not contribute to the overall meaning of the text.\n",
    "   - Convert the text to lowercase to ensure consistency and avoid treating the same words as different based on their case.\n",
    "   - Tokenize the text by splitting it into individual words or tokens. This step separates the text into its fundamental units for further analysis.\n",
    "   - Remove stop words (commonly occurring words like \"is,\" \"the,\" \"and\") as they often do not carry much semantic meaning.\n",
    "\n",
    "2. Text Normalization and Lemmatization:\n",
    "   - Normalize the words by applying stemming or lemmatization techniques. Stemming reduces words to their base or root form, while lemmatization converts words to their dictionary or base form.\n",
    "   - This step helps to consolidate words with the same meaning, reducing the vocabulary size and improving the accuracy of subsequent analyses.\n",
    "\n",
    "3. Feature Extraction and Representation:\n",
    "   - Convert the preprocessed text into numerical representations that machine learning algorithms can understand.\n",
    "   - Use techniques like bag-of-words (BoW) or term frequency-inverse document frequency (TF-IDF) to represent the text as a numerical matrix.\n",
    "   - BoW represents each document as a vector of word frequencies, while TF-IDF takes into account the importance of words in the context of the entire corpus.\n",
    "   - Additionally, you can explore more advanced techniques like word embeddings (e.g., Word2Vec or GloVe) that capture semantic relationships between words.\n",
    "\n",
    "By following these steps, you can transform raw text into a structured and meaningful representation that captures the essence of the original text. This enables you to perform various natural language processing (NLP) tasks such as sentiment analysis, text classification, or information retrieval."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 25
   },
   "source": [
    "#### Answer:\n",
    "\n",
    "- Lowercase (standardize case)\n",
    "- Remove stopwords (really common words that likely have no semantic value)\n",
    "- Stem or lemmatize to remove prefixes/suffixes/grammer bits\n",
    "- Remove punctuation\n",
    "- Tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Describe what vectorized text would look like as a dataframe.\n",
    "\n",
    "If you vectorize the above corpus, what would the rows and columns be in the resulting dataframe (aka document term matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataframe (document-term matrix) would have rows representing each document and columns representing unique words from the corpus, with values indicating word frequencies in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized text, when represented as a dataframe, typically has each document or text sample represented as a row, and each feature or word represented as a column. The values in the dataframe correspond to the frequency, occurrence, or weight of each word in the respective document.\n",
    "\n",
    "Here's an example to illustrate the structure of a vectorized text dataframe:\n",
    "\n",
    "```plaintext\n",
    "| Document | word1 | word2 | word3 | ... | wordN |\n",
    "|----------|-------|-------|-------|-----|-------|\n",
    "|    1     |   2   |   0   |   1   | ... |   3   |\n",
    "|    2     |   0   |   1   |   0   | ... |   2   |\n",
    "|    3     |   1   |   0   |   2   | ... |   0   |\n",
    "```\n",
    "\n",
    "In this example, we have three documents represented as rows, and each column represents a specific word or feature in the text. The values in the cells indicate the frequency or weight of each word in the corresponding document.\n",
    "\n",
    "It's worth noting that vectorized text dataframes can have different representations based on the specific encoding technique used, such as bag-of-words, TF-IDF, or word embeddings. The example above represents the bag-of-words approach, where each cell represents the frequency of a word in the document.\n",
    "\n",
    "This dataframe representation allows for easy integration with machine learning algorithms since the textual data is transformed into a structured numerical format that algorithms can process and analyze."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you vectorize the given corpus using the bag-of-words approach, the resulting document-term matrix (dataframe) would have the rows representing each document in the corpus and the columns representing each unique word (term) found in the corpus.\n",
    "\n",
    "In this specific case, the resulting document-term matrix (dataframe) would look as follows:\n",
    "\n",
    "```plaintext\n",
    "| Document | about | adult | best | book | earth | excuse | heard | harry | is | it | like | me | non-fiction | only | person | potter | read | sea | the | to | um | wizards | young |\n",
    "|----------|-------|-------|------|------|-------|--------|-------|-------|----|----|------|----|-------------|------|--------|---------|------|-----|-----|----|----|---------|-------|\n",
    "|    1     |   1   |   1   |   1  |   1  |   0   |   0    |   0   |   1   |  1 |  0 |   0  | 0  |     0       |  0   |   0    |    1    |  0   |  0  |  1  |  0 | 0  |    1    |   1   |\n",
    "|    2     |   0   |   0   |   0  |   0  |   1   |   1    |   1   |   0   |  0 |  0 |   0  | 0  |     0       |  0   |   0    |    0    |  0   |  1  |   1 |  0 | 1  |    0    |   0   |\n",
    "|    3     |   0   |   0   |   0  |   0  |   0   |   0    |   0   |   0   |  1 |  1 |   1  | 1  |     1       |  1   |   1    |    0    |  1   |  0  |   0 |  1 | 0  |    0    |   0   |\n",
    "```\n",
    "\n",
    "Each row represents a document from the corpus (sentence_one, sentence_two, sentence_three), and each column represents a unique word (term) found in the corpus. The values in the cells indicate the frequency of each word in the respective document.\n",
    "\n",
    "Please note that in this example, I have assumed that the words in the corpus have been preprocessed and tokenized, resulting in a specific set of unique terms. The actual resulting dataframe would depend on the preprocessing steps applied to the raw text before vectorization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 25
   },
   "source": [
    "#### Answer:\n",
    "\n",
    "- Columns: every word/token in the dataset/corpus\n",
    "- Rows: the documents you're vectorizing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: What does TF-IDF do?\n",
    "\n",
    "Also, what does TF-IDF stand for?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that measures the importance of a term in a document by considering its frequency within the document and rarity across a corpus. It helps identify relevant and distinctive terms in natural language processing tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF stands for \"Term Frequency-Inverse Document Frequency.\" It is a numerical statistic used in natural language processing and information retrieval to measure the importance of a term in a document relative to a corpus of documents.\n",
    "\n",
    "TF-IDF combines two components:\n",
    "\n",
    "1. Term Frequency (TF): Measures the frequency of a term in a document. It indicates how often a term appears in a specific document. A higher term frequency suggests that the term is more relevant to that document.\n",
    "\n",
    "2. Inverse Document Frequency (IDF): Measures the rarity or uniqueness of a term across the corpus. It quantifies the importance of a term by considering how often it appears in the entire corpus. Rare terms that appear in few documents receive a higher IDF score, indicating their potential significance.\n",
    "\n",
    "The TF-IDF score of a term in a document is calculated by multiplying the term frequency (TF) and the inverse document frequency (IDF). The resulting score emphasizes terms that are frequent within a document but rare across the entire corpus.\n",
    "\n",
    "The purpose of TF-IDF is to highlight terms that are both relevant to a specific document and distinctive in the overall context of the corpus. It helps in identifying important terms and distinguishing them from commonly occurring terms. TF-IDF is widely used in various natural language processing tasks, such as text classification, information retrieval, and document similarity analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 25
   },
   "source": [
    "#### Answer:\n",
    "\n",
    "- TF-IDF: term frequency inverse document frequency\n",
    "- TF-IDF is a vectorizer that takes into account the rarity of the words\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 33
   },
   "source": [
    "## NLP in Code\n",
    "\n",
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "index": 34
   },
   "outputs": [],
   "source": [
    "# New section, new data\n",
    "policies = pd.read_csv('data/2020_policies_feb_24.csv')\n",
    "\n",
    "def warren_not_warren(label):\n",
    "    \n",
    "    '''Make label a binary between Elizabeth Warren\n",
    "    speeches and speeches from all other candidates'''\n",
    "    \n",
    "    if label =='warren':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "policies['candidate'] = policies['candidate'].apply(warren_not_warren)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 35
   },
   "source": [
    "The dataframe loaded above consists of policies of 2020 Democratic presidential hopefuls. The `policy` column holds text describing the policies themselves.  The `candidate` column indicates whether it was or was not an Elizabeth Warren policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "index": 36
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>policy</th>\n",
       "      <th>candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100% Clean Energy for America</td>\n",
       "      <td>As published on Medium on September 3rd, 2019:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A Comprehensive Agenda to Boost America’s Smal...</td>\n",
       "      <td>Small businesses are the heart of our economy....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A Fair and Welcoming Immigration System</td>\n",
       "      <td>As published on Medium on July 11th, 2019:\\r\\n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A Fair Workweek for America’s Part-Time Workers</td>\n",
       "      <td>Working families all across the country are ge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A Great Public School Education for Every Student</td>\n",
       "      <td>I attended public school growing up in Oklahom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               name  \\\n",
       "0           0                      100% Clean Energy for America   \n",
       "1           1  A Comprehensive Agenda to Boost America’s Smal...   \n",
       "2           2            A Fair and Welcoming Immigration System   \n",
       "3           3    A Fair Workweek for America’s Part-Time Workers   \n",
       "4           4  A Great Public School Education for Every Student   \n",
       "\n",
       "                                              policy  candidate  \n",
       "0  As published on Medium on September 3rd, 2019:...          1  \n",
       "1  Small businesses are the heart of our economy....          1  \n",
       "2  As published on Medium on July 11th, 2019:\\r\\n...          1  \n",
       "3  Working families all across the country are ge...          1  \n",
       "4  I attended public school growing up in Oklahom...          1  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policies.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 37
   },
   "source": [
    "The documents for activity are in the `policy` column, and the target is candidate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, 4)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policies.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Import the Relevant Class, Then Instantiate and Fit a Count Vectorizer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First! Train-test split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Code here to train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(policies['policy'], policies['candidate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate it\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit it\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Vectorize Your Text, Then Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "index": 42
   },
   "outputs": [],
   "source": [
    "# Code here to transform train and test sets with the vectorizer\n",
    "X_tr_vec = vectorizer.transform(X_train)\n",
    "X_te_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "index": 44
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the classifier...\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Code here to instantiate and fit a Random Forest model\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_tr_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8541666666666666"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code here to evaluate your model on the test set\n",
    "rfc.score(X_te_vec, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "# 4) Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "## Clustering Concepts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 1: Describe how the K-Means algorithm updates its cluster centers after initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "#### Answer:\n",
    "\n",
    "- You set the number of cluster centers (K) - algorithm randomly starts with that number of cluster centers (in random spots!)\n",
    "- The algorithm calculates the distance between the centers and each observation and assigns the observation to the closest cluster center to create the first iteration of clusters\n",
    "- The algorithm then takes all the observations assigned to each cluster, and moves that cluster center to be at the exact actual center (mean) of the newly created cluster\n",
    "- Repeat! Until the cluster centers stop moving (or tolerance is met - some parameters in the implementation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 2: What is inertia, and how does K-Means use inertia to determine the best estimator?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "Please also describe the method you can use to evaluate clustering using inertia.\n",
    "\n",
    "Documentation, for reference: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "#### Answer:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "- Inertia measures the distance between each point and its center - the idea is that better clusters are more tightly concentrated\n",
    "- KMeans tries to minimize inertia when choosing cluster centers\n",
    "- Method to evaluate - elbow plot!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 3: What other metric do we have to score the clusters which are formed?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "Describe the difference between it and inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "#### Answer:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "- Silhouette score\n",
    "- Difference between silhouette score and inertia: silhouette score tries to maximize similarity within groups and maximize distances between clusters, while inertia just looks within each cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "## Clustering in Code with Heirarchical Agglomerative Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "After the above conceptual review of KMeans, let's practice coding with agglomerative clustering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# New dataset for this section!\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data['data'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 4: Prepare our Data for Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "What steps do we need to take to preprocess our data effectively?\n",
    "\n",
    "- scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# Code to preprocess the data\n",
    "k_scaler = StandardScaler()\n",
    "\n",
    "# Name the processed data X_processed\n",
    "X_processed = k_scaler.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 5: Import the Relevant Class, Then Instantiate and Fit a Hierarchical Agglomerative Clustering Object"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "Let's use `n_clusters = 2` to start (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "index": 83
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5770346019475988"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the relevent clustering algorithm\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Instantiate\n",
    "cluster = AgglomerativeClustering(n_clusters=2)\n",
    "# Fit the object\n",
    "cluster.fit(X_processed)\n",
    "\n",
    "# Calculate a silhouette score\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(X_processed, cluster.labels_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 6: Write a Function to Test Different Options for `n_clusters`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "The function should take in the number for `n_clusters` and the data to cluster, fit a new clustering model using that parameter to the data, print the silhouette score, then return the labels attribute from the fit clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "index": 83
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5770346019475988\n",
      "0.4466890410285909\n",
      "0.4006363159855973\n",
      "0.33058726295230545\n",
      "0.31485480100512825\n",
      "0.316969830299128\n",
      "0.310946529007258\n"
     ]
    }
   ],
   "source": [
    "def test_n_for_clustering(n, data):\n",
    "    \"\"\" \n",
    "    Tests different numbers for the hyperparameter n_clusters\n",
    "    Prints the silhouette score for that clustering model\n",
    "    Returns the labels that are output from the clustering model\n",
    "\n",
    "    Parameters: \n",
    "    -----------\n",
    "    n: float object\n",
    "        number of clusters to use in the agglomerative clustering model\n",
    "    data: Pandas DataFrame or array-like object\n",
    "        Data to cluster\n",
    "\n",
    "    Returns: \n",
    "    --------\n",
    "    labels: array-like object\n",
    "        Labels attribute from the clustering model\n",
    "    \"\"\"\n",
    "    # Create the new clustering model\n",
    "    cluster = AgglomerativeClustering(n_clusters=n)\n",
    "    \n",
    "    # Fit the new clustering model\n",
    "    cluster.fit(data)\n",
    "\n",
    "    # Print the silhouette score\n",
    "    print(silhouette_score(data, cluster.labels_))\n",
    "    \n",
    "    # Return the labels attribute from the fit clustering model\n",
    "    return cluster.labels_\n",
    "\n",
    "# Testing your function\n",
    "\n",
    "for n in range(2, 9):\n",
    "    test_n_for_clustering(n, X_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
